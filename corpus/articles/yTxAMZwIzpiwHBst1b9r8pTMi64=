Go to HBR.org 

3/4 REMAINING  REGISTER | SUBSCRIBE  

ORGANIZATIONAL CULTURE
A Brief History of Decision Making
Leigh BuchananAndrew O'Connell
FROM THE JANUARY 2006 ISSUE
    8.95
Sometime in the midst of the last century, Chester Barnard, a retired telephone executive and author of The Functions of the Executive, imported the term “decision making” from the lexicon of public administration into the business world. There it began to replace narrower descriptors such as “resource allocation” and “policy making.”

The introduction of that phrase changed how managers thought about what they did and spurred a new crispness of action and desire for conclusiveness, argues William Starbuck, professor in residence at the University of Oregon’s Charles H. Lundquist College of Business. “Policy making could go on and on endlessly, and there are always resources to be allocated,” he explains. “‘Decision’ implies the end of deliberation and the beginning of action.”

So Barnard—and such later theorists as James March, Herbert Simon, and Henry Mintzberg—laid the foundation for the study of managerial decision making. But decision making within organizations is only one ripple in a stream of thought flowing back to a time when man, facing uncertainty, sought guidance from the stars. The questions of who makes decisions, and how, have shaped the world’s systems of government, justice, and social order. “Life is the sum of all your choices,” Albert Camus reminds us. History, by extrapolation, equals the accumulated choices of all mankind.

The study of decision making, consequently, is a palimpsest of intellectual disciplines: mathematics, sociology, psychology, economics, and political science, to name a few. Philosophers ponder what our decisions say about ourselves and about our values; historians dissect the choices leaders make at critical junctures. Research into risk and organizational behavior springs from a more practical desire: to help managers achieve better outcomes. And while a good decision does not guarantee a good outcome, such pragmatism has paid off. A growing sophistication with managing risk, a nuanced understanding of human behavior, and advances in technology that support and mimic cognitive processes have improved decision making in many situations.

Even so, the history of decision-making strategies is not one of unalloyed progress toward perfect rationalism. In fact, over the years we have steadily been coming to terms with constraints—both contextual and psychological—on our ability to make optimal choices. Complex circumstances, limited time, and inadequate mental computational power reduce decision makers to a state of “bounded rationality,” argues Simon. While Simon suggests that people would make economically rational decisions if only they could gather enough information, Daniel Kahneman and Amos Tversky identify factors that cause people to decide against their economic interest even when they know better. Antonio Damasio draws on work with brain-damaged patients to demonstrate that in the absence of emotion it is impossible to make any decisions at all. Erroneous framing, bounded awareness, excessive optimism: the debunking of Descartes’s rational man threatens to swamp our confidence in our choices, with only improved technology acting as a kind of empirical breakwater.

Faced with the imperfectability of decision making, theorists have sought ways to achieve, if not optimal outcomes, at least acceptable ones. Gerd Gigerenzer urges us to make a virtue of our limited time and knowledge by mastering simple heuristics, an approach he calls “fast and frugal” reasoning. Amitai Etzioni proposes “humble decision making,” an assortment of nonheroic tactics that include tentativeness, delay, and hedging. Some practitioners, meanwhile, have simply reverted to the old ways. Last April, a Japanese television equipment manufacturer turned over its $20 million art collection to Christie’s when the auction house trounced archrival Sotheby’s in a high-powered round of rock-paper-scissors, a game that may date back as far as Ming Dynasty China.

In this special issue on decision making, our focus—as always—is on breaking new ground. What follows is a glimpse of the bedrock that lies beneath that ground.

Chances Are

Risk is an inescapable part of every decision. For most of the everyday choices people make, the risks are small. But on a corporate scale, the implications (both upside and downside) can be enormous. Even the tritely expressed (and rarely encountered) win-win situation entails opportunity costs in the form of paths not taken.

To make good choices, companies must be able to calculate and manage the attendant risks. Today, myriad sophisticated tools can help them do so. But it was only a few hundred years ago that the risk management tool kit consisted of faith, hope, and guesswork. That’s because risk is a numbers game, and before the seventeenth century, humankind’s understanding of numbers wasn’t up to the task.

A History of Choice








































































































READ MORE
Most early numbering methods were unwieldy, as anyone knows who has tried to multiply XXIII by VI. The Hindu-Arabic numeral system (which, radically, included zero) simplified calculations and enticed philosophers to investigate the nature of numbers. The tale of our progression from those early fumblings with base 10 is masterfully told by Peter Bernstein in Against the Gods: The Remarkable Story of Risk.

Bernstein’s account begins in the dark days when people believed they had no control over events and so turned to priests and oracles for clues to what larger powers held in store for them. It progresses quickly to a new interest in mathematics and measurement, spurred, in part, by the growth of trade. During the Renaissance, scientists and mathematicians such as Girolamo Cardano mused about probability and concocted puzzles around games of chance. In 1494, a peripatetic Franciscan monk named Luca Pacioli proposed “the problem of points”—which asks how one should divide the stakes in an incomplete game. Some 150 years later, French mathematicians Blaise Pascal and Pierre de Fermat developed a way to determine the likelihood of each possible result of a simple game (balla, which had fascinated Pacioli).

But it wasn’t until the next century, when Swiss scholar Daniel Bernoulli took up the study of random events, that the scientific basis for risk management took shape.

Bernoulli (who also introduced the far-reaching concept of human capital) focused not on events themselves but on the human beings who desire or fear certain outcomes to a greater or lesser degree. His intent, he wrote, was to create mathematical tools that would allow anyone to “estimate his prospects from any risky undertaking in light of [his] specific financial circumstances.” In other words, given the chance of a particular outcome, how much are you willing to bet?

In the nineteenth century, other scientific disciplines became fodder for the risk thinkers. Carl Friedrich Gauss brought his geodesic and astronomical research to bear on the bell curve of normal distribution. The insatiably curious Francis Galton came up with the concept of regression to the mean while studying generations of sweet peas. (He later applied the principle to people, observing that few of the sons—and fewer of the grandsons—of eminent men were themselves eminent.)

But it wasn’t until after World War I that risk gained a beachhead in economic analysis. In 1921, Frank Knight distinguished between risk, when the probability of an outcome is possible to calculate (or is knowable), and uncertainty, when the probability of an outcome is not possible to determine (or is unknowable)—an argument that rendered insurance attractive and entrepreneurship, in Knight’s words, “tragic.” Some two decades later, John von Neumann and Oskar Morgenstern laid out the fundamentals of game theory, which deals in situations where people’s decisions are influenced by the unknowable decisions of “live variables” (aka other people).

Today, of course, corporations try to know as much as is humanly and technologically possible, deploying such modern techniques as derivatives, scenario planning, business forecasting, and real options. But at a time when chaos so often triumphs over control, even centuries’ worth of mathematical discoveries can do only so much. Life “is a trap for logicians,” wrote the novelist G.K. Chesterton. “Its wildness lies in wait.”

The Meeting of Minds

In the fifth century BC, Athens became the first (albeit limited) democracy. In the seventeenth century, the Quakers developed a decision-making process that remains a paragon of efficiency, openness, and respect. Starting in 1945, the United Nations sought enduring peace through the actions of free peoples working together.

There is nobility in the notion of people pooling their wisdom and muzzling their egos to make decisions that are acceptable—and fair—to all. During the last century, psychologists, sociologists, anthropologists, and even biologists (studying everything from mandrills to honeybees) eagerly unlocked the secrets of effective cooperation within groups. Later, the popularity of high-performance teams, coupled with new collaborative technologies that made it “virtually” impossible for any man to be an island, fostered the collective ideal.

The scientific study of groups began, roughly, in 1890, as part of the burgeoning field of social psychology. In 1918, Mary Parker Follett made a passionate case for the value of conflict in achieving integrated solutions in The New State: Group Organization—The Solution of Popular Government. A breakthrough in understanding group dynamics occurred just after World War II, sparked—oddly enough—by the U.S. government’s wartime campaign to promote the consumption of organ meat. Enlisted to help, psychologist Kurt Lewin discovered that people were more likely to change their eating habits if they thrashed the subject out with others than if they simply listened to lectures about diet. His influential “field theory” posited that actions are determined, in part, by social context and that even group members with very different perspectives will act together to achieve a common goal.

Over the next decades, knowledge about group dynamics and the care and feeding of teams evolved rapidly. Victor Vroom and Philip Yetton established the circumstances under which group decision making is appropriate. R. Meredith Belbin defined the components required for successful teams. Howard Raiffa explained how groups exploit “external help” in the form of mediators and facilitators. And Peter Drucker suggested that the most important decision may not be made by the team itself but rather by management about what kind of team to use.

Meanwhile, research and events collaborated to expose collective decision making’s dark underbelly. Poor group decisions—of the sort made by boards, product development groups, management teams—are often attributed to the failure to mix things up and question assumptions. Consensus is good, unless it is achieved too easily, in which case it becomes suspect. Irving Janis coined the term “groupthink” in 1972 to describe “a mode of thinking that people engage in when they are deeply involved in a cohesive in-group, when the members’ strivings for unanimity override their motivation to realistically appraise alternative courses of action.” In his memoir, A Thousand Days, former Kennedy aide Arthur Schlesinger reproached himself for not objecting during the planning for the Bay of Pigs invasion: “I can only explain my failure to do more than raise a few timid questions by reporting that one’s impulse to blow the whistle on this nonsense was simply undone by the circumstances of the discussion.”

Consensus is good, unless it is achieved too easily, in which case it becomes suspect.

It seems that decisions reached through group dynamics require, above all, a dynamic group. As Clarence Darrow neatly put it: “To think is to differ.”

Thinking Machines

Computer professionals eulogize Xerox PARC of the 1970s as a technological Eden where some of today’s indispensable tools sprouted. But comparable vitality and progress were evident two decades earlier at the Carnegie Institute of Technology in Pittsburgh. There, a group of distinguished researchers laid the conceptual—and in some cases the programming—foundation for computer-supported decision making.

Future Nobel laureate Herbert Simon, Allen Newell, Harold Guetzkow, Richard M. Cyert, and James March were among the CIT scholars who shared a fascination with organizational behavior and the workings of the human brain. The philosopher’s stone that alchemized their ideas was electronic computing. By the mid-1950s, transistors had been around less than a decade, and IBM would not launch its groundbreaking 360 mainframe until 1965. But already scientists were envisioning how the new tools might improve human decision making. The collaborations of these and other Carnegie scientists, together with research by Marvin Minsky at the Massachusetts Institute of Technology and John McCarthy of Stanford, produced early computer models of human cognition—the embryo of artificial intelligence.

AI was intended both to help researchers understand how the brain makes decisions and to augment the decision-making process for real people in real organizations. Decision support systems, which began appearing in large companies toward the end of the 1960s, served the latter goal, specifically targeting the practical needs of managers. In a very early experiment with the technology, managers used computers to coordinate production planning for laundry equipment, Daniel Power, editor of the Web site DSSResources.com, relates. Over the next decades, managers in many industries applied the technology to decisions about investments, pricing, advertising, and logistics, among other functions. 

But while technology was improving operational decisions, it was still largely a cart horse for hauling rather than a stallion for riding into battle. Then in 1979, John Rockart published the HBR article “Chief Executives Define Their Own Data Needs,” proposing that systems used by corporate leaders ought to give them data about the key jobs the company must do well to succeed. That article helped launch “executive information systems,” a breed of technology specifically geared toward improving strategic decision making at the top. In the late 1980s, a Gartner Group consultant coined the term “business intelligence” to describe systems that help decision makers throughout the organization understand the state of their company’s world. At the same time, a growing concern with risk led more companies to adopt complex simulation tools to assess vulnerabilities and opportunities.

In the 1990s, technology-aided decision making found a new customer: customers themselves. The Internet, which companies hoped would give them more power to sell, instead gave consumers more power to choose from whom to buy. In February 2005, the shopping search service BizRate reports, 59% of online shoppers visited aggregator sites to compare prices and features from multiple vendors before making a purchase, and 87% used the Web to size up the merits of online retailers, catalog merchants, and traditional retailers.

In the 1990s, technology-aided decision making found a new customer: customers themselves.

Unlike executives making strategic decisions, consumers don’t have to factor what Herbert Simon called “zillions of calculations” into their choices. Still, their newfound ability to make the best possible buying decisions may amount to technology’s most significant impact to date on corporate success—or failure.

The Romance of the Gut

“Gut,” according to the first definition in Merriam-Webster’s latest edition, means “bowels.” But when Jack Welch describes his “straight from the gut” leadership style, he’s not talking about the alimentary canal. Rather, Welch treats the word as a conflation of two slang terms: “gut” (meaning emotional response) and “guts” (meaning fortitude, nerve).

That semantic shift—from human’s stomach to lion’s heart—helps explain the current fascination with gut decision making. From our admiration for entrepreneurs and firefighters, to the popularity of books by Malcolm Gladwell and Gary Klein, to the outcomes of the last two U.S. presidential elections, instinct appears ascendant. Pragmatists act on evidence. Heroes act on guts. As Alden Hayashi writes in “When to Trust Your Gut” (HBR February 2001): “Intuition is one of the X factors separating the men from the boys.”

We don’t admire gut decision makers for the quality of their decisions so much as for their courage in making them. Gut decisions testify to the confidence of the decision maker, an invaluable trait in a leader. Gut decisions are made in moments of crisis when there is no time to weigh arguments and calculate the probability of every outcome. They are made in situations where there is no precedent and consequently little evidence. Sometimes they are made in defiance of the evidence, as when Howard Schultz bucked conventional wisdom about Americans’ thirst for a $3 cup of coffee and Robert Lutz let his emotions guide Chrysler’s $80 million investment in a $50,000 muscle car. Financier George Soros claims that back pains have alerted him to discontinuities in the stock market that have made him fortunes. Such decisions are the stuff of business legend.

Decision makers have good reasons to prefer instinct. In a survey of executives that Jagdish Parikh conducted when he was a student at Harvard Business School, respondents said they used their intuitive skills as much as they used their analytical abilities, but they credited 80% of their successes to instinct. Henry Mintzberg explains that strategic thinking cries out for creativity and synthesis and thus is better suited to intuition than to analysis. And a gut is a personal, nontransferable attribute, which increases the value of a good one. Readers can parse every word that Welch and Lutz and Rudolph Giuliani write. But they cannot replicate the experiences, thought patterns, and personality traits that inform those leaders’ distinctive choices.

A gut is a personal, nontransferable attribute, which increases the value of a good one.

Although few dismiss outright the power of instinct, there are caveats aplenty. Behavioral economists such as Daniel Kahneman, Robert Shiller, and Richard Thaler have described the thousand natural mistakes our brains are heir to. And business examples are at least as persuasive as behavioral studies. Michael Eisner (Euro Disney), Fred Smith (ZapMail), and Soros (Russian securities) are among the many good businesspeople who have made bad guesses, as Eric Bonabeau points out in his article “Don’t Trust Your Gut” (HBR May 2003).

Of course the gut/brain dichotomy is largely false. Few decision makers ignore good information when they can get it. And most accept that there will be times they can’t get it and so will have to rely on instinct. Fortunately, the intellect informs both intuition and analysis, and research shows that people’s instincts are often quite good. Guts may even be trainable, suggest John Hammond, Ralph Keeney, Howard Raiffa, and Max Bazerman, among others.

In The Fifth Discipline, Peter Senge elegantly sums up the holistic approach: “People with high levels of personal mastery…cannot afford to choose between reason and intuition, or head and heart, any more than they would choose to walk on one leg or see with one eye.” A blink, after all, is easier when you use both eyes. And so is a long, penetrating stare.
A version of this article appeared in the January 2006 issue of Harvard Business Review.
Leigh Buchanan is a former senior editor at HBR.
Andrew O’Connell, an editor with the Harvard Business Review Group, is the author of Stats and Curiosities from Harvard Business Review.
This article is about ORGANIZATIONAL CULTURE

 FOLLOW THIS TOPIC
 
UP NEXT IN BEHAVIORAL ECONOMICS
From "Economic Man" to Behavioral Economics

Justin Fox
 
UP NEXT IN DECISION MAKING
Who Has the D?: How Clear Decision Roles Enhance Organizational Performance

Marcia W. Blenko; Paul Rogers
 
UP NEXT IN FORECASTING
The Hidden Traps in Decision Making

Howard Raiffa; John S. Hammond; Ralph L. Keeney
Comments


POST
3 COMMENTS
Ricardo Mateus 3 months ago

Instinct is not the same as intuition, although the terms have been used interchangeably across the text.
Good summary though!

REPLY 0  0  JOIN THE CONVERSATION
POSTING GUIDELINES
We hope the conversations that take place on HBR.org will be energetic, constructive, and thought-provoking. To comment, readers must sign in or register. And to ensure the quality of the discussion, our moderating team will review all comments and may edit them for clarity, length, and relevance. Comments that are overly promotional, mean-spirited, or off-topic may be deleted per the moderators' judgment. All postings become the property of Harvard Business Publishing.

Partner Center


Subscribe + Save!
EXPLORE HBR

HBR STORE

ABOUT HBR

HBR SUBSCRIBER ASSISTANCE

HBR.ORG CUSTOMER ASSISTANCE

FOLLOW HBR

Facebook
Twitter
LinkedIn
Google+
Your Newsreader
 `A Brief History of Decision Making
